{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0I0-lcWM1CX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "company=[]\n",
        "def get_company_names(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "\n",
        "    if response.status_code == 200:\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        company_elements = soup.find_all('h2', class_='fw-extrabold fs-xl hover-underline d-inline-block company-title-clamp mb-0')\n",
        "\n",
        "\n",
        "        company_names = [element.text.strip() for element in company_elements]\n",
        "        return company_names\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def extract_all_company_names(base_url, num_pages):\n",
        "    all_company_names = []\n",
        "\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}?page={page_number}\"\n",
        "        company_names = get_company_names(url)\n",
        "\n",
        "        if company_names:\n",
        "            all_company_names.extend(company_names)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return all_company_names\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://builtin.com/companies/tech/aws-companies\"\n",
        "    num_pages = 1                                                        #change the number of pages to get 1000 companies(change to 44)\n",
        "    all_company_names = extract_all_company_names(base_url, num_pages)\n",
        "\n",
        "    for i, company_name in enumerate(all_company_names, start=1):\n",
        "        company.append(company_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_company_names(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all table rows (tr)\n",
        "        table_rows = soup.find_all('tr')\n",
        "\n",
        "        # Extract and print the company names from the second column (td)\n",
        "        company_names = [row.find_all('td')[0].text.strip() for row in table_rows if row.find_all('td')]\n",
        "        return company_names\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.oceanfrogs.com/cloud/aws/list-of-companies-using-aws/\"\n",
        "\n",
        "    company_names = get_company_names(url)\n",
        "\n",
        "    # Print the extracted company names\n",
        "    for i, company_name in enumerate(company_names, start=1):\n",
        "        company.append(company_name)"
      ],
      "metadata": {
        "id": "JTQlw-mQPzCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_company_names(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the <em> tag that contains the company names\n",
        "        em_tag = soup.find('em')\n",
        "\n",
        "        # Extract and return the company names\n",
        "        if em_tag:\n",
        "            company_names = [name.strip() for name in em_tag.text.split(',')]\n",
        "            return company_names\n",
        "        else:\n",
        "            print(\"No <em> tag found on the page.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.contino.io/insights/whos-using-aws\"\n",
        "\n",
        "    company_names = extract_company_names(url)\n",
        "\n",
        "    # Print the extracted company names\n",
        "    for i, company_name in enumerate(company_names, start=1):\n",
        "        company.append(company_name)"
      ],
      "metadata": {
        "id": "k2hX-Xf8P6Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googlesearch import search\n",
        "import csv\n",
        "\n",
        "def get_website_link(company):\n",
        "    try:\n",
        "        # Perform a Google search and get the first result\n",
        "        for result in search(f\"{company} official website\", num=1, stop=1):\n",
        "            return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "# Store company names and links in a list\n",
        "data_for_csv = []\n",
        "\n",
        "# Counter for processed companies\n",
        "count_processed = 0\n",
        "\n",
        "# Iterate through the list of companies and get website links\n",
        "for current_company in company:\n",
        "    website_link = get_website_link(current_company)\n",
        "    data_for_csv.append([current_company, website_link])\n",
        "\n",
        "    # Increment the counter\n",
        "    count_processed += 1\n",
        "\n",
        "    # Display count in output\n",
        "    print(f\"{count_processed} \")\n",
        "\n",
        "# Specify the CSV file name\n",
        "csv_filename = 'company_data.csv'\n",
        "\n",
        "# Write data to CSV file\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Company Name', 'Website Link'])\n",
        "    for row in data_for_csv:\n",
        "        writer.writerow(row)\n",
        "\n",
        "# Print final message with the total count\n",
        "print(f\"\\nData for {count_processed} companies has been written to {csv_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE12Ai-oRMdK",
        "outputId": "bf75222a-fe16-4406-89cd-02d46fb61971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \n",
            "2 \n",
            "3 \n",
            "4 \n",
            "5 \n",
            "6 \n",
            "7 \n",
            "8 \n",
            "9 \n",
            "10 \n",
            "11 \n",
            "12 \n",
            "13 \n",
            "14 \n",
            "15 \n",
            "16 \n",
            "17 \n",
            "18 \n",
            "19 \n",
            "20 \n",
            "\n",
            "Data for 20 companies has been written to company_data.csv\n"
          ]
        }
      ]
    }
  ]
}